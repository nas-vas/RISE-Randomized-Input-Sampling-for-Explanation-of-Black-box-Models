{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a89fb7-48dc-43ea-887f-ff7314940745",
   "metadata": {},
   "source": [
    "# RISE: Randomized Input Sampling for Explanation of Black-box Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330e2ab-4e16-45f5-9b62-ba984714dc2c",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The paper addresses the challenge of Explainable AI for deep neural networks employed in image classification. Acknowledging the opacity of these models, particularly in conveying their decision-making process, the authors introduce RISE (Randomized Input Sampling for Explanation), a novel approach to enhance interpretability. RISE focuses on black-box models that take images as input and produce class probabilities as output. Unlike white-box methods that rely on internal network states, RISE employs a unique strategy by generating importance maps through empirical estimation. This involves probing the model with randomly masked versions of input images and observing the resulting outputs. The proposed method is evaluated against state-of-the-art importance extraction techniques using both automatic deletion/insertion metrics and pointing metrics based on human-annotated object segments. Through extensive experiments on various benchmark datasets, RISE demonstrates comparable or superior performance compared to existing methods, including white-box approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baed17b-9466-4456-91b4-12f9861dd1f5",
   "metadata": {},
   "source": [
    "## 1 Indtroduction\n",
    "\n",
    "Acknowledging the significant strides of deep neural networks in AI research, the paper underscores the critical issue of understanding and explaining the decision-making process of these complex models. This challenge becomes particularly crucial in high-stakes domains like medical diagnosis and autonomous driving. In response to the need for transparent and trustworthy models, the authors propose an Explainable AI (XAI) approach. Using the popular image classification model ResNet50, the paper illustrates instances where the model's predictions raise questions. The proposed solution is RISE, a black-box explanation approach that operates without internal access to the model, making it adaptable to various network architectures.\n",
    "\n",
    "The authors provide a comparative analysis, drawing attention to the limitations of existing methods such as LIME, which relies on superpixels. RISE stands out by employing random masks to probe the base model, record responses, and generate an importance map without internal access. This innovative approach allows insights into arbitrary networks without compromising internal information. Another significant contribution is the introduction of causal metrics, aiming to transcend traditional human-centric evaluations. The paper critiques these evaluations and introduces deletion and insertion metrics as more objective measures of explanation quality. These metrics assess the drop and rise in class probabilities as important pixels are removed or added, providing a more human-agnostic and causal evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c29ce-fe19-49a5-9b58-f7fcaccce745",
   "metadata": {},
   "source": [
    "## 2 Related works\n",
    "\n",
    "The importance of providing explanations has been widely explored across various disciplines, including machine learning. Traditional methods, such as using rules or decision trees, have proven interpretable to humans. Another research avenue involves approximating less interpretable models, like neural networks or nonlinear SVMs, with simpler, interpretable models such as decision rules or sparse linear models. Ribeiro et al. introduced LIME, an approximate linear decision model, but its application to black-box networks, including the discussed approach, led to subpar importance maps due to reliance on superpixels.\n",
    "\n",
    "In the context of explaining image classification decisions, existing approaches ground image regions supporting decisions visually or generate textual descriptions. Interpretability in neural network explanation often involves designing interpretable network architectures or justifying decisions made by existing models. Some models achieve interpretability by incorporating changes to a white-box base model, like Xu et al.'s image captioning system, or by employing specific network architectures.\n",
    "\n",
    "Neural justification approaches aim to justify a base model's decision. Third-person models train additional models based on human-annotated ground truth reasoning, producing saliency maps or textual justifications. These methods rely on tediously labeled ground-truth explanations and may not offer high-fidelity explanations. In contrast, first-person models, including the discussed approach, generate explanations without additional models. Various methods visualize internal representations learned by CNNs, synthesize inputs that activate neurons, or use strategies like Class Activation Mapping (CAM) and Grad-CAM to compute location-specific importance. However, these methods assume access to the internals of the base model. The discussed approach, RISE, stands out as a more general framework, obtaining the importance map solely based on the input and output of the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c134f8d-489f-41f6-8453-c0cbdcb46501",
   "metadata": {},
   "source": [
    "## 3 Randomized Input Sampling for Explanation (RISE)\n",
    "\n",
    "An approach to assessing the significance of an image region involves perturbing or obscuring it and observing the impact on the black box decision. Techniques such as setting pixel intensities to zero, blurring the region, or adding noise can be employed for this purpose. In the presented work, the importance of pixels is evaluated by dimming them randomly, reducing their intensities to zero. This is achieved by multiplying the image with a mask having values in the range of [0, 1].\n",
    "\n",
    "Let $f : I → \\mathbb{R}$ be a black-box model, that for a given input from $I$ produces scalar confidence score. In our case, $I$ is the space of color images $I = {I | I : Λ →\\mathbb{R}^3}$ of size $H × W$\n",
    "\n",
    "To mitigate potential adversarial effects caused by independently masking pixels, the paper addresses the challenge of significant confidence score variation resulting from slight changes in pixel values. Additionally, the method avoids the limitations of generating binary masks with elements set to zeros and ones, which would lead to a mask space of size $2H×W$. To overcome these issues, the authors adopt a strategy of sampling smaller binary masks and subsequently upsampling them to a larger resolution using bilinear interpolation. This approach ensures a smooth importance map $(S)$ without introducing sharp edges. The final masks $(Mi)$ are characterized by values in the [0, 1] range, offering more flexibility. The summarized mask generation process involves sampling $N$ binary masks of size $h × w$ (smaller than the image size H × W), upscaling them to $(h + 1)CH × (w + 1)CW$ through bilinear interpolation, and cropping areas $H × W$ with uniformly random indents within the limits of $(CH, CW)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89508b8b-0972-4f0d-9fc9-61833eedbf78",
   "metadata": {},
   "source": [
    "## 4. Experiments\n",
    "\n",
    "RISE was assessed using three widely used object classification datasets: PASCAL VOC07, MSCOCO2014, and ImageNet. The evaluation involved testing the importance maps generated by various explanation methods for a target object category within images from VOC and MSCOCO datasets. In the case of the ImageNet dataset, explanations were tested for the top probable class of each image. To ensure a fair comparison with existing literature, specific versions of VOC and MSCOCO datasets were selected, and ResNet50 and VGG16 networks trained by previous work were used as base models for these datasets. The base models for the ImageNet dataset were downloaded from the PyTorch model zoo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb17ffc-6ca2-4a0e-af47-9f69f8e5c91d",
   "metadata": {},
   "source": [
    "### 4.1 Evaluation Metrics\n",
    "\n",
    "Despite the increasing focus on explainable machine learning, there remains a lack of consensus on how to measure a model's explainability. Human evaluation, assessing transparency, user trust, and comprehension of model decisions, has been the predominant method. Existing methods evaluate saliency maps based on object localization, which might not capture the true causes behind a model's decision. To address this, the paper introduces two automatic evaluation metrics: deletion and insertion. Deletion measures the decrease in the probability of the predicted class as important pixels are removed, while insertion measures the increase in probability as pixels are introduced. These metrics aim to be objective, free from human bias, and efficient in terms of time and resources.\n",
    "\n",
    "The deletion metric involves removing pixels, aiming to force the base model to change its decision. The insertion metric involves introducing pixels, with higher AUC indicating a better explanation. Different strategies for pixel removal and introduction are considered, each with its pros and cons. The pointing game, a human evaluation metric, assesses whether the highest saliency point lies inside the human-annotated bounding box of an object. This metric provides insights into the model's reliance on objects for classification, and a high pointing game accuracy is indicative of a good explanation. The proposed metrics aim to overcome the limitations of existing evaluation methods and provide a more objective assessment of model explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463af9b1-ddf7-4538-951a-ff475f020fb1",
   "metadata": {},
   "source": [
    "## 4.2 Experimental Results\n",
    "\n",
    "In the experimental settings, binary random masks are generated with equal probabilities for 0's and 1's. Different numbers of masks are empirically selected for various CNN classifiers, such as 4000 for VGG16 and 8000 for ResNet50, with consistent dimensions $(h = w = 7, H = W = 224)$. Comparison results are either obtained from published works or by running publicly available code.\n",
    "\n",
    "Regarding deletion and insertion scores on the ImageNet val split, Table 1 \n",
    "\n",
    "<img src=\"./Summary_IMG/Table_1.png\" width=\"800\"> \n",
    "\n",
    "<img src=\"./Summary_IMG/Table_1.png\" width=\"300\">\n",
    "\n",
    "demonstrates RISE's comparative evaluation against state-of-the-art approaches. RISE outperforms others, including the white-box Grad-CAM method, providing better performance for ResNet50. Despite increased computational load due to the number of forward passes, RISE's performance superiority is attributed to its classification model. However, noise in importance maps may occur, especially with objects of varying sizes.\n",
    "\n",
    "Pointing game accuracy on the test split of PASCAL VOC07 and val split of MSCOCO2014 is presented in Table 2. RISE, as the only black-box method, consistently outperforms white-box methods, particularly excelling with VGG16 on the VOC dataset. For ResNet50, RISE's pointing accuracy competes well, showcasing its competitiveness despite being black-box and more general than methods like CAM. Low standard deviation values highlight the robustness of RISE against mask randomness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88c0ed-8833-4984-a9c3-3a1bb0d499a5",
   "metadata": {},
   "source": [
    "## 4.3 RISE for Captioning\n",
    "\n",
    "RISE's applicability extends to explaining captions in image description systems. While existing works often rely on separate attention networks or access to feature activations and/or gradient values, RISE offers a unique approach. A comparable method, Ramanishka et al., probes the base model with conv features from fixed-size patches, unlike RISE, which adapts to varying object sizes and captures additional context in the image. An example demonstrates RISE's application in explaining image captions using a base captioning model, showcasing its versatility. The base model computes the probability of the next word given the partial sentence and the input image. RISE probes the model through randomly masked inputs and computes saliency, providing explanations for each word in the sentence. \n",
    "\n",
    "Fig 5 displays three instances of such explanations for an MSCOCO image.\n",
    "\n",
    "<img src=\"./Summary_IMG/Fig5.png\" width=\"800\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330608e0-9c28-4d58-916c-8000c5d7a884",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "The paper introduced RISE as an explanation approach for black-box models, estimating the importance of input image regions in model predictions. Despite its simplicity and versatility, RISE demonstrates superior performance in automatic causal metrics and competitive results in human-centric pointing metrics compared to existing approaches. The authors suggest future work to leverage RISE's generality in explaining decisions across complex networks in video and other domains. Acknowledgments include partial support from the DARPA XAI program."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
